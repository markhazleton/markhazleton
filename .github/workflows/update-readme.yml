name: Update README with Latest Blog Posts

on:
  schedule:
    # Runs at 12:00 AM UTC every day
    - cron: "0 0 * * *"
  workflow_dispatch: # Allows manual trigger of the workflow
    inputs:
      debug_mode:
        description: 'Enable detailed debug output'
        required: false
        default: false
        type: boolean

jobs:
  update-readme:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Install xmlstarlet
        run: sudo apt-get install -y xmlstarlet

      - name: Fetch Latest Blog Posts
        id: fetch_blog_posts
        run: |
          echo "Fetching RSS feed with bot-friendly approach..."
          
          # Try multiple approaches to bypass Cloudflare bot protection
          success=false
          
          # Method 1: Real browser User-Agent with proper headers
          echo "Trying with real browser headers..."
          if curl -s --fail --connect-timeout 15 --max-time 45 \
            -H "User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36" \
            -H "Accept: application/rss+xml, application/xml, text/xml" \
            -H "Accept-Language: en-US,en;q=0.9" \
            -H "Accept-Encoding: gzip, deflate, br" \
            -H "DNT: 1" \
            -H "Connection: keep-alive" \
            -H "Upgrade-Insecure-Requests: 1" \
            https://markhazleton.com/rss.xml -o rss.xml; then
            if grep -q "<rss\|<feed" rss.xml 2>/dev/null; then
              echo "Successfully downloaded RSS with browser headers"
              success=true
            fi
          fi
          
          # Method 2: Try with delay and different user agent
          if [ "$success" = "false" ]; then
            echo "Trying with delay and RSS reader user agent..."
            sleep 3
            if curl -s --fail --connect-timeout 15 --max-time 45 \
              -H "User-Agent: FeedBurner/1.0 (http://www.FeedBurner.com)" \
              -H "Accept: application/rss+xml, application/xml, text/xml" \
              https://markhazleton.com/rss.xml -o rss_alt1.xml; then
              if grep -q "<rss\|<feed" rss_alt1.xml 2>/dev/null; then
                mv rss_alt1.xml rss.xml
                echo "Successfully downloaded RSS with FeedBurner user agent"
                success=true
              fi
            fi
          fi
          
          # Method 3: Try wget with real browser agent
          if [ "$success" = "false" ]; then
            echo "Trying wget with browser user agent..."
            sleep 2
            if wget --timeout=30 --tries=1 --quiet \
              --user-agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36" \
              --header="Accept: application/rss+xml, application/xml, text/xml" \
              -O rss_alt2.xml https://markhazleton.com/rss.xml; then
              if grep -q "<rss\|<feed" rss_alt2.xml 2>/dev/null; then
                mv rss_alt2.xml rss.xml
                echo "Successfully downloaded RSS with wget"
                success=true
              fi
            fi
          fi
          
          # Check if we got valid RSS content
          if [ "$success" = "false" ]; then
            echo "All methods failed to download valid RSS content"
            echo "This appears to be due to Cloudflare bot protection"
            echo "Consider adding GitHub Actions IPs to your Cloudflare allowlist"
            exit 1
          fi
          
          # Verify the file has content and is valid XML
          if [ ! -s rss.xml ]; then
            echo "RSS file is empty"
            exit 1
          fi
          
          echo "RSS file downloaded successfully ($(wc -c < rss.xml) bytes)"
          
          # Parse the RSS feed
          echo "Parsing RSS feed..."
          if ! xmlstarlet sel -t -m '//item' -v 'concat("- [", title, "](", link, ")")' -n rss.xml | head -8 > latest-posts.md; then
            echo "Failed to parse RSS feed"
            echo "First few lines of downloaded file:"
            head -5 rss.xml
            exit 1
          fi
          
          # Verify parsing worked
          if [ ! -s latest-posts.md ]; then
            echo "No blog posts found or RSS feed is empty"
            exit 1
          fi
          
          echo "Successfully parsed RSS feed:"
          cat latest-posts.md

      - name: Update README.md
        run: |
          # Read the latest posts into a variable
          latest_posts=$(<latest-posts.md)

          # Replace the content between the <!-- BLOG-POST-LIST:START --> and <!-- BLOG-POST-LIST:END --> tags
          awk -v latest_posts="$latest_posts" '
          BEGIN {in_blog_list=0}
          /<!-- BLOG-POST-LIST:START -->/ {print; print latest_posts; in_blog_list=1; next}
          /<!-- BLOG-POST-LIST:END -->/ {print; in_blog_list=0; next}
          !in_blog_list {print}
          ' README.md > updated_readme.md

      - name: Check if README.md was updated
        id: check_changes
        run: |
          # Compare updated README.md with the current one
          if ! diff updated_readme.md README.md > /dev/null; then
            mv updated_readme.md README.md
            echo "changes_detected=true" >> $GITHUB_ENV
          else
            echo "README.md is up to date. No changes needed."
            echo "changes_detected=false" >> $GITHUB_ENV
            exit 0
          fi

      - name: Clean up untracked files
        run: |
          rm -f latest-posts.md updated_readme.md rss.xml

      - name: Commit Changes
        if: env.changes_detected == 'true'
        run: |
          git config --global user.name "GitHub README Updater"
          git config --global user.email "noreply@markhazleton.com"
          git add README.md
          git commit -m "Updated README with latest blog posts from markhazleton.com"
          git push
